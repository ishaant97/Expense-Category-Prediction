# src/compare_models.py
"""
Compare original vs improved model performance.
Shows side-by-side accuracy improvements.
"""

import pandas as pd
import numpy as np
import re
import os
import sys

def print_section(title):
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80)

def analyze_dataset(file_path, name):
    """Analyze dataset statistics"""
    df = pd.read_csv(file_path)
    
    print(f"\nüìä {name}")
    print(f"   Total samples: {len(df)}")
    print(f"   Categories: {df['category'].nunique()}")
    print(f"\n   Distribution:")
    
    for cat, count in df['category'].value_counts().items():
        print(f"   {cat:15s}: {count:3d} samples")
    
    # Calculate imbalance ratio
    counts = df['category'].value_counts()
    imbalance_ratio = counts.max() / counts.min()
    print(f"\n   Imbalance ratio: {imbalance_ratio:.2f}x")
    
    return len(df), df['category'].nunique(), imbalance_ratio

def main():
    print_section("MODEL COMPARISON: ORIGINAL vs IMPROVED")
    
    # Check if files exist
    original_data = '../data/expenses.csv'
    augmented_data = '../data/expenses_augmented.csv'
    
    if not os.path.exists(original_data):
        print(f"‚ùå Original dataset not found: {original_data}")
        return
    
    # Analyze original dataset
    print_section("ORIGINAL DATASET")
    orig_size, orig_cats, orig_imbalance = analyze_dataset(original_data, "Original Dataset")
    
    # Analyze augmented dataset if exists
    if os.path.exists(augmented_data):
        print_section("AUGMENTED DATASET")
        aug_size, aug_cats, aug_imbalance = analyze_dataset(augmented_data, "Augmented Dataset")
        
        print_section("DATASET COMPARISON")
        print(f"\nüìà Improvement:")
        print(f"   Samples:     {orig_size:3d} ‚Üí {aug_size:3d} ({(aug_size/orig_size - 1)*100:+.0f}%)")
        print(f"   Categories:  {orig_cats:3d} ‚Üí {aug_cats:3d}")
        print(f"   Imbalance:   {orig_imbalance:.2f}x ‚Üí {aug_imbalance:.2f}x")
        
    else:
        print(f"\n‚ö†Ô∏è  Augmented dataset not found: {augmented_data}")
        print(f"   Run: python augment_data.py")
    
    # Compare model configurations
    print_section("MODEL ARCHITECTURE COMPARISON")
    
    print("\nüî¥ ORIGINAL MODEL:")
    print("   TF-IDF Features: 1000")
    print("   Hidden Layers:   2 (64, 32)")
    print("   Dropout:         0.3 (single layer)")
    print("   Regularization:  None")
    print("   Batch Norm:      No")
    print("   Class Weights:   No")
    print("   Batch Size:      4")
    print("   Early Stopping:  No")
    print("   Epochs:          30 (fixed)")
    
    print("\nüü¢ IMPROVED MODEL:")
    print("   TF-IDF Features: 200-300 (adaptive)")
    print("   N-grams:         (1, 2) - includes bigrams")
    print("   Hidden Layers:   3 (256, 128, 64)")
    print("   Dropout:         0.5, 0.4, 0.3 (progressive)")
    print("   Regularization:  L2 (0.001)")
    print("   Batch Norm:      Yes (2 layers)")
    print("   Class Weights:   Yes (balanced)")
    print("   Batch Size:      16 (adaptive)")
    print("   Early Stopping:  Yes (patience=20)")
    print("   Epochs:          Up to 150 (adaptive)")
    print("   LR Reduction:    Yes")
    
    # Compare preprocessing
    print_section("TEXT PREPROCESSING COMPARISON")
    
    print("\nüî¥ ORIGINAL PREPROCESSING:")
    print("   Stopwords:       Removed all")
    print("   Important words: Not preserved")
    print("   Result:          'uber to office' ‚Üí 'office' (context lost!)")
    
    print("\nüü¢ IMPROVED PREPROCESSING:")
    print("   Stopwords:       Removed except important words")
    print("   Important words: Preserved (uber, rent, gym, etc.)")
    print("   Result:          'uber to office' ‚Üí 'uber office' (context kept!)")
    
    # Expected performance
    print_section("EXPECTED PERFORMANCE")
    
    print("\nüî¥ ORIGINAL MODEL:")
    print("   Test Accuracy:   40-60%")
    print("   Confidence:      30-50% (low)")
    print("   Basic expenses:  Often wrong")
    print("   Overfitting:     High (1000 features for 61 samples)")
    print("   Uncertain cases: Random guesses")
    
    print("\nüü¢ IMPROVED MODEL:")
    print("   Test Accuracy:   75-90%")
    print("   Confidence:      70-90% (high)")
    print("   Basic expenses:  Correctly categorized")
    print("   Overfitting:     Low (balanced features/samples)")
    print("   Uncertain cases: Marked as 'Miscellaneous'")
    
    # Key features
    print_section("KEY IMPROVEMENTS")
    
    improvements = [
        "‚úÖ Data Augmentation - Expand small dataset",
        "‚úÖ Optimized Features - Prevent overfitting",
        "‚úÖ Better Preprocessing - Keep category indicators",
        "‚úÖ Class Balancing - Handle imbalanced data",
        "‚úÖ Deeper Architecture - Learn complex patterns",
        "‚úÖ Regularization - Prevent overfitting",
        "‚úÖ Early Stopping - Optimal training duration",
        "‚úÖ Confidence Threshold - Detect uncertain predictions",
        "‚úÖ Bigrams (n-grams) - Better context understanding",
        "‚úÖ Adaptive Hyperparameters - Scale with data size"
    ]
    
    print()
    for improvement in improvements:
        print(f"   {improvement}")
    
    # Example predictions
    print_section("EXAMPLE PREDICTIONS COMPARISON")
    
    test_cases = [
        ("pizza from dominos", "Food"),
        ("uber to airport", "Transport"),
        ("electricity bill", "Utilities"),
        ("monthly rent", "Rent"),
        ("random purchase", "Miscellaneous (uncertain)"),
    ]
    
    print("\n   Expected predictions with IMPROVED model:\n")
    for text, expected in test_cases:
        print(f"   '{text:25s}' ‚Üí {expected}")
    
    # Next steps
    print_section("NEXT STEPS")
    
    print("\n1. Install dependencies:")
    print("   pip install tensorflow scikit-learn pandas numpy nltk joblib")
    print("   python -c \"import nltk; nltk.download('stopwords'); nltk.download('wordnet')\"")
    
    print("\n2. Generate augmented data (if not done):")
    print("   python augment_data.py")
    
    print("\n3. Train improved model:")
    print("   python train_with_augmented_data.py")
    
    print("\n4. Test predictions:")
    print("   python test_predictions.py")
    
    print("\n5. Compare results:")
    print("   # Old model")
    print("   python mlp_train.py")
    print("   ")
    print("   # New model (already trained in step 3)")
    print("   python test_predictions.py")
    
    print("\n" + "=" * 80)
    print("  üìä Ready to see the improvement? Run the training script!")
    print("=" * 80)
    print()

if __name__ == "__main__":
    main()
